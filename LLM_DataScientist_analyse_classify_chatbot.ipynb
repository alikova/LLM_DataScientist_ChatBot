{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["21mXR5shtZCU","mZdg82zJXUoD","Gw26eQA6QzxW","U76FVw4dQ9jn","pOL84caqRGJb"],"authorship_tag":"ABX9TyPPEpBtvQr7Hc/hBN8EVlTo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Goal is to analyze and classify messages, then develop a chatbot that can respond to natural language queries with meaningful insights.\n","\n","Records in the dataset contains:\n","- id_user = unique 'int' value\n","- timestamp = 'str' -- convert it to timestamp\n","- source = 'str'\n","- message 'str'"],"metadata":{"id":"qfx-hP85GBUY"}},{"cell_type":"markdown","metadata":{"id":"j9NLS82cGq5F"},"source":["## Mount Google Drive"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m6qZjDDI-ENG","executionInfo":{"status":"ok","timestamp":1744221530024,"user_tz":-120,"elapsed":31277,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"34fb9c59-95aa-4a16-f74e-e2549e3c4457"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Instalations"],"metadata":{"id":"21mXR5shtZCU"}},{"cell_type":"code","source":["!pip install PyGithub"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jk9JAnVVtbED","executionInfo":{"status":"ok","timestamp":1744221624258,"user_tz":-120,"elapsed":5243,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"3608fe89-d841-4cf6-e1a4-95f8f38c9166"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting PyGithub\n","  Downloading PyGithub-2.6.1-py3-none-any.whl.metadata (3.9 kB)\n","Collecting pynacl>=1.4.0 (from PyGithub)\n","  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n","Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (2.32.3)\n","Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (4.13.1)\n","Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (2.3.0)\n","Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from PyGithub) (1.2.18)\n","Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n","Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pynacl>=1.4.0->PyGithub) (1.17.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (2025.1.31)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->PyGithub) (1.17.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.22)\n","Downloading PyGithub-2.6.1-py3-none-any.whl (410 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pynacl, PyGithub\n","Successfully installed PyGithub-2.6.1 pynacl-1.5.0\n"]}]},{"cell_type":"markdown","source":["## Git Commit"],"metadata":{"id":"mZdg82zJXUoD"}},{"cell_type":"code","source":["# Step 1: Reset the environment by forcing a directory change to a safe location\n","import os\n","os.chdir('/content')\n"],"metadata":{"id":"eeUSrMNBXW9V","executionInfo":{"status":"ok","timestamp":1744272479110,"user_tz":-120,"elapsed":19,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Step 2: Clean up any existing problematic directories\n","!rm -rf /content/LLM_DataScientist_ChatBot\n","!rm -rf /content/repo\n"],"metadata":{"id":"97hPvE9uvNpd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 3: Mount Google Drive properly\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n"],"metadata":{"id":"mXyi7BjpvQc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 4: Set up the GitHub token correctly\n","# This needs to be done manually through Colab's secrets manager\n","# or directly in this cell (but be careful with security)\n","import getpass\n","import os\n","\n","# Get token securely (won't be visible in the output)\n","github_token = getpass.getpass('Enter your GitHub token: ')\n","os.environ['GITHUB_TOKEN'] = github_token\n","\n","# Verify the token is set\n","print(f\"Token is set: {os.environ.get('GITHUB_TOKEN') is not None}\")\n"],"metadata":{"id":"CNQtQBDGvSTr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 5: Now clone the repository\n","!git clone https://github.com/alikova/LLM_DataScientist_ChatBot.git /content/clean_repo\n"],"metadata":{"id":"uEeUAen1vU1w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 6: Navigate to the repository\n","%cd /content/clean_repo\n"],"metadata":{"id":"UmiajcbTvW4Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 7: Configure Git\n","!git config --global user.name \"alikova\"\n","!git config --global user.email \"z.alenka7@gmail.com\"\n"],"metadata":{"id":"-EHmJBW3vYOy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 8: List files in Google Drive to find the notebook\n","!ls -la \"/content/drive/MyDrive/Colab Notebooks/\"\n"],"metadata":{"id":"chSvtxssvaS2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 9: Once you find the correct path, copy the notebook\n","# Update this path based on the ls command output\n","# !cp \"/content/drive/MyDrive/Colab Notebooks/YOUR_ACTUAL_PATH/LLM_DataScientist_analyse_classify_chatbot.ipynb\" .\n","\n","# Step 10: Add, commit, and push using the token\n","# !git add LLM_DataScientist_analyse_classify_chatbot.ipynb\n","# !git commit -m \"Add analysis and classification notebook\"\n","# !git push https://$GITHUB_TOKEN@github.com/alikova/LLM_DataScientist_ChatBot.git main"],"metadata":{"id":"hJ7WNGQuvb2q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Preprocessing and Cleaning"],"metadata":{"id":"KxFsJVrfOggU"}},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer\n","\n","df = pd.read_csv(\"LLM-DataScientist-Task_Data.csv\")\n","\n","def clean_text(text):\n","  text = text.lower()\n","  text = re.sub(r'\\s+', ' ', text)\n","  text = re.sub(r'[^\\w\\s]', '', text)\n","  return text\n","\n","# Apply cleaning function\n","df['cleaned_message'] = df['message'].apply(clean_text)\n","\n","# Tokenization\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","df['tokens'] = df['cleaned_message'].apply(lambda x: tokenizer.tokenize(x))\n","\n","df.to_csv(\"cleaned_LLM-DataScientist-Task_Data.csv\", index=False)"],"metadata":{"id":"u-Ou-d0HOgGo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Message Vectorization with HuggingFace Transformers\n","- pre-trained transformer model from HuggingFace ---> for converting the text messages into numerical vectors,\n","- BERT or HuggingFace ---> for encoding text messages into embeddings for later classification,\n","- fine-tuning the transformer model\n","- Use the embeddings as features for downstream classification /\n","\n","Metric for Evaluation:\n","- accuracy, precision and recall, F1-score, confusion matrix"],"metadata":{"id":"hO8EyQnpOrQa"}},{"cell_type":"code","source":["from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n","import torch\n","\n","# Load pre-trained BERT model\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n","\n","# Tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize the messages\n","def tokenize_function(examples):\n","    return tokenizer(examples['cleaned_message'], padding=\"max_length\", truncation=True)\n","\n","# Tokenize dataset\n","encoded_dataset = df['cleaned_message'].apply(lambda x: tokenize_function({'cleaned_message': x}))\n","\n","# Split into train/test sets\n","train_data, test_data = train_test_split(encoded_dataset, test_size=0.2)\n","\n","# Set up the Trainer\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=16,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=10,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_data,\n","    eval_dataset=test_data\n",")\n","\n","# Train the model\n","trainer.train()\n"],"metadata":{"id":"FzQPiXW2O27o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report, confusion_matrix\n","\n","# Get predictions\n","predictions = trainer.predict(test_data)\n","predicted_labels = predictions.predictions.argmax(axis=-1)\n","\n","# Evaluation metrics\n","print(classification_report(test_data, predicted_labels))\n","print(confusion_matrix(test_data, predicted_labels))\n"],"metadata":{"id":"v8yCkHxwwhOq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Message Categorization and Filtering (Classification and Categorization of Messages)\n","\n","- Categories: Once the model is trained, use the transformer model's output (predicted category) to classify each message\n","- Filtering: Based on time range and source, filter the messages before feeding them into the model"],"metadata":{"id":"hZeucMVnP6R3"}},{"cell_type":"code","source":["# Assuming categories are ['login issues', 'game issues', 'payment issues', etc.]\n","df['predicted_category'] = model.predict(df['cleaned_message'])\n"],"metadata":{"id":"lVSIajWgQCDz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_filtered = df[(df['timestamp'] >= '2023-01-01') & (df['source'] == 'telegram')]\n"],"metadata":{"id":"vGmyAj-qwljv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Building the Chatbot PetE - Pete Bot"],"metadata":{"id":"eTegsN4kQfbO"}},{"cell_type":"code","source":["def chatbot_response(query):\n","    # Process the query (e.g., extract the category, time range, and source)\n","    category = extract_category_from_query(query)\n","    time_range = extract_time_range_from_query(query)\n","    source = extract_source_from_query(query)\n","\n","    # Filter data based on the query\n","    filtered_data = filter_data(df, category, time_range, source)\n","\n","    # Return the response\n","    return generate_response(filtered_data)\n","\n","# Example function call\n","response = chatbot_response(\"Show me all login issues from the last month.\")\n","print(response)\n"],"metadata":{"id":"Cd6rI014Qvph"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Deploy and Tests"],"metadata":{"id":"769N7-_5QwHs"}},{"cell_type":"markdown","source":["### Test\n","\n","- testing the model with different messages and evaluate its accuracy"],"metadata":{"id":"Gw26eQA6QzxW"}},{"cell_type":"code","source":[],"metadata":{"id":"KpxBu9AvQ9EW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Deploy\n","\n","- deoply the finished model and chatbot to a server/cloud"],"metadata":{"id":"U76FVw4dQ9jn"}},{"cell_type":"code","source":[],"metadata":{"id":"Uq08v-wERFg_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Document\n","\n","- update the GitHub repository with detailed documentation, including the setup instructions, usage, and how to interact with the chatbot"],"metadata":{"id":"pOL84caqRGJb"}},{"cell_type":"code","source":[],"metadata":{"id":"kBpAKobMRcMB"},"execution_count":null,"outputs":[]}]}